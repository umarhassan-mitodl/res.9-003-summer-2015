---
content_type: video_gallery
description: This page summarizes the unit topic and activities, and links to lecture
  videos, notes and further study resources.
draft: false
is_media_gallery: true
learning_resource_types: []
ocw_type: SupplementalResourceSection
title: Unit 9. Theory of Intelligence
uid: a9e7f99a-ce34-3b66-1440-d2ef1d026bdf
video_metadata:
  youtube_id: null
videos:
  content:
  - a4f05057-174c-8897-29a8-66b2cf2a5444
  - 9ae61743-f111-cd71-002e-cfe229cd917b
  - ae879ea8-4b89-756d-456d-a68887dfbbdb
  website: res-9-003-brains-minds-and-machines-summer-course-summer-2015
---
## Unit Overview

{{< tableopen >}}{{< tbodyopen >}}{{< tropen >}}{{< tdopen >}}
{{< resource uuid="ea34d43d-037f-bd1b-ec4d-c20f3e97567e" >}}
{{< tdclose >}}{{< tdopen >}}
Tomaso Poggio and his colleagues have developed a model of the early processing stages in the ventral visual pathway of the brain, which may underlie our ability to recognize object categories from visual input in a brief flash of less than 100 milliseconds. (Courtesy of Tomaso Poggio and Thomas Serre. From "[Models of visual cortex](http://www.scholarpedia.org/article/Models_of_visual_cortex)." *Scholarpedia* 8 no. 4 (2013): 3516. License CC BY-NC-SA.)
{{< tdclose >}}{{< trclose >}}{{< tbodyclose >}}{{< tableclose >}}

Research on learning in deep networks has led to impressive performance by machines on tasks such as object recognition, but a deep understanding of the behavior of these networks and why they perform so well remains a mystery. In this unit, you will first learn about a model of rapid object recognition in visual cortex that resembles the structure of deep networks. You will then learn some of the theory behind how the structural connectivity, complexity, and dynamics of deep networks govern their learning behavior.

Tomaso Poggio describes a theory of processing in the ventral pathway of the brain that solves the challenging problem of recognizing objects despite variations in their visual appearance due to geometric transformations such as translation and rotation.

The guest lecture by Surya Ganguli shows how insights from statistical mechanics applied to the analysis of high-dimensional data can contribute to our understanding of how functions such as object categorization emerge in multi-layer neural networks.

Haim Sompolinsky explores the theoretical role of common properties of neural architectures in biological systems, in learning tasks such as classification. These properties include the number of stages of the neural network, compression or expansion of the dimensionality of the information, the role of noise, and presence of recurrent or feedback connections.

## Unit Activities

### Useful Background

- Introductions to statistics and machine learning, including deep learning networks

### Videos and Slides

{{< video-gallery "a9e7f99a-ce34-3b66-1440-d2ef1d026bdf" >}}

## Further Study

Additional information about the speakers' research and publications can be found at their websites:

- [Surya Ganguli, Neural Dynamics and Computation Lab, Stanford](https://ganguli-gang.stanford.edu/)
- [Tomaso Poggio, MIT](http://cbcl.mit.edu/)
- [Haim Sompolinsky, Harvard and Neurophysics Lab, Hebrew University of Jerusalem](https://elsc.huji.ac.il/people-directory/faculty-members/haim-sompolinsky/)

Advani, M., and S. Ganguli. ["Statistical Mechanics of High-Dimensional Inference." (PDF)](http://ganguli-gang.stanford.edu/pdf/HighDimInf.pdf) (2016).

Anselmi, F., J. Z. Leibo, et al. "[Unsupervised Learning of Invariant Representations](https://www.researchgate.net/profile/Joel_Leibo/publication/281139622_Unsupervised_learning_of_invariant_representations/links/562d1ca408aef25a244314a6)." *Theoretical Computer Science* 633 (2016): 112–21.

Babadi, B., and H. Sompolinsky. ["Sparseness and Expansion in Sensory Representations."](https://www.sciencedirect.com/science/article/pii/S0896627314006461) *Neuron* 83 (2014): 1213–26.

Gao, P., and S. Ganguli. ["On Simplicity and Complexity in the Brave New World of Large-Scale Neuroscience." (PDF)](http://ganguli-gang.stanford.edu/pdf/15.BraveNewWorld.pdf) *Current Opinion in Neurobiology* 32 (2015): 148–55.

Poggio, T. ["Deep Learning: Mathematics and Neuroscience." (PDF - 1.2MB)](http://cbmm.mit.edu/sites/default/files/publications/Deep%20Learning-%20mathematics%20and%20neuroscience.pdf) *Center for Brains Minds & Machines* Views & Reviews (2016).

Saxe, A., J. McClelland, et al. ["Learning Hierarchical Category Structure in Deep Neural Networks." (PDF)](http://ganguli-gang.stanford.edu/pdf/Saxe.13.HierCat.pdf) *Proceedings 35th Annual Meeting of the Cognitive Science Society* (2013): 1271–76.

Serre, T., G. Kreiman, et al. ["A Quantitative Theory of Immediate Visual Recognition." (PDF)](http://klab.tch.harvard.edu/publications/PDFs/gk2329.pdf) *Progress in Brain Research* 165 (2007): 33–56.

Sompolinsky, H. ["Computational Neuroscience: Beyond the Local Circuit."](https://pubmed.ncbi.nlm.nih.gov/24602868/) (PDF) Opinion in Neurobiology 2014 Sompolinsky.pdf) *Current Opinion in Neurobiology* 25 (2014): 1–6.