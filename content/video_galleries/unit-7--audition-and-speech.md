---
content_type: video_gallery
description: This page summarizes the unit topic and activities, and links to lecture
  videos, notes and further study resources.
draft: false
is_media_gallery: true
learning_resource_types: []
ocw_type: SupplementalResourceSection
title: Unit 7. Audition and Speech
uid: 67de5f18-8df4-42c1-f171-08d83925c7a8
video_metadata:
  youtube_id: null
videos:
  content:
  - b1cdcd1c-0ff5-0f9c-498b-e92f74007790
  - 7ed95b15-ff85-3b73-c6c9-b99eda5106a8
  - b329b06b-9a17-2e11-33f1-dfa3b6408570
  - a0491f33-e279-4cc7-a03c-4563c6f7d5f8
  - e29386f3-d919-bf43-aa08-b55bf653b7f6
  - 2222f900-180c-c0d2-f2fb-aade5088a985
  website: res-9-003-brains-minds-and-machines-summer-course-summer-2015
---
## Unit Overview

{{< tableopen >}}{{< tbodyopen >}}{{< tropen >}}{{< tdopen >}}
{{< resource uuid="c184198a-ec04-736f-c5ee-84b08da84cbd" >}}
{{< tdclose >}}{{< tdopen >}}

How do we recognize the source of a sound texture, such as the sound of insects or waves? Josh McDermott and colleagues propose a model of this process that uses statistics of the frequency content of the sounds, and the modulation of this content over time, depicted in these spectrograms (bottom) and plots of modulation power (top).

(Courtesy of Elsevier, Inc., {{% resource_link "61b53d22-d7d0-4047-972f-d4443959f446" "http://www.sciencedirect.com" %}}. Used with permission. Source: McDermott, Josh H., and Eero P. Simoncelli. "Sound texture perception via statistics of the auditory periphery: evidence from sound synthesis." *Neuron* 71, no. 5 (2011): 926-940.)

{{< tdclose >}}{{< trclose >}}{{< tbodyclose >}}{{< tableclose >}}

How do we use the auditory signals generated by the ear to recognize events in the world, such as running water, a crackling fire, music, or conversation between speakers? In this unit, you will learn about the structure and function of the auditory system, the nature of sound, and how it can be used to recognize sound textures and speech. Physiologically inspired models of these processes capture the neural mechanisms by which the brain processes this information.

Part 1 of Josh McDermott's lecture provides an overview of the auditory system and neural encoding of sound, and explores the problem of recognizing sound textures. A model of this process exploits statistical properties of the frequency content of an incoming sound.

Part 2 of Josh McDermott's lecture delves more deeply into the ability of human listeners to recognize sound textures, using the synthesis of artificial sounds as a powerful tool to test a model of this process. The lecture also examines other cues used to analyze the content of a scene from the sounds that it produces.

Nancy Kanwisher presents fMRI studies that led to the discovery of regions of auditory cortex that are specialized for the analysis of particular classes of sound such as music and speech.

Hynek Hermansky addresses the problem of speech processing, beginning with the structure of speech signals, how they are generated by a speaker, how speech is initially processed in the brain, and key aspects of auditory perception. The lecture then reviews the history of speech recognition in machines.

Part 2 of Hynek Hermansky's lecture examines the key challenge of recognizing speech in a way that is invariant to large speaker variations and unwanted noise in the auditory signal, and how insights from human audition can inform models of speech processing.

A panel of experts in vision and hearing reflect on the similarities and differences between these two modalities, and how exploitation of the synergies between the two may accelerate the pace of research on the study of vision and audition in brains and machines.

## Unit Activities

### Useful Background

- Introductions to neuroscience and statistics
- The lecture by Hynek Hermansky lecture requires background in signal processing

### Videos and Slides

{{< video-gallery "67de5f18-8df4-42c1-f171-08d83925c7a8" >}}

## Further Study

Additional information about the speakers' research and publications can be found at their websites:

- {{% resource_link "f216de57-e73e-4c37-a042-8639ea508d94" "Josh McDermott, MIT" %}}
- {{% resource_link "5e1243aa-2bf6-4dab-8607-28682067d43a" "Nancy Kanwisher, MIT" %}}
- {{% resource_link "1b84ef52-241a-4cbb-a0cf-79c4b9ab89cd" "Hynek Hermansky, Johns Hopkins University" %}}
- {{% resource_link "1c6969a4-7dac-47ce-804e-00af03b07a7e" "Dan Yamins, MIT" %}}

Hermansky, H., J. R. Cohen, et al. {{% resource_link "43b77702-38ce-41c2-ba60-e0e4e29ca2bb" "\"Perceptual Properties of Current Speech Recognition Technology.\" (PDF - 1.7MB)" %}} *Proceedings of the IEEE* 101, no. 9 (2013): 1968–85.

McDermott, J. H. {{% resource_link "f4267e9d-f8fe-4b48-bf72-d36ce9aa3e73" "\"The Cocktail Party Problem.\" (PDF)" %}} *Current Biology* 19, no. 22 (2009): R1024–27.

———. {{% resource_link "1bfed243-a28e-4b68-811f-ec7087dda435" "\"Audition.\" (PDF - 1.1MB)" %}} In *Oxford Handbook of Cognitive Neuroscience Two Volume Set (Oxford Library of Psychology)*, Edited by K. N. Ochsner and S. Kosslyn. Oxford University Press, 2013. ISBN: 9780195381597.

McDermott, J. H., M. Schemitsch, et al. {{% resource_link "90486969-0e13-4fe4-a570-10137dba09f6" "\"Summary Statistics in Auditory Perception.\" (PDF - 2.9MB)" %}} *Nature Neuroscience* 16, no. 4 (2013): 493–98.

McDermott, J. H., and E. P. Simoncelli. "{{% resource_link "ee1a94d8-a48a-4085-9fa2-8da391df8dd2" "Sound Texture Perception via Statistics of the Auditory Periphery: Evidence from Sound Synthesis" %}}." *Neuron* 71 (2011): 926–40.

Norman-Haignere, S., N. Kanwisher, et al. {{% resource_link "b560b4fe-ed85-4e9a-ac6d-8352b1b8fb59" "\"Distinct Cortical Pathways for Music and Speech Revealed by Hypothesis-Free Voxel Decomposition.\" (PDF - 10.3MB)" %}} *Neuron* 88, no. 6 (2015): 1281–96.

Yamins, D., and J. J. DiCarlo. "{{% resource_link "caf95141-6109-4e97-b1d4-d47ddc492194" "Using Goal-Driven Deep Learning Models to Understand Sensory Cortex" %}}." *Nature Neuroscience* 19, no. 3 (2016): 356–65.